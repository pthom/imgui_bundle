{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c41d18",
   "metadata": {},
   "source": [
    "# AI Training with Real-Time Tuning and Visualization\n",
    "\n",
    "This notebook demonstrates how Dear ImGui Bundle can be used in a Jupyter Notebook in order to provide a real-time interface\n",
    "for tuning hyperparameters and visualizing the training process of a neural network.\n",
    "\n",
    "**Features:**\n",
    "- Live training loss visualization\n",
    "- Adjustable hyperparameters during training (learning rate, momentum)\n",
    "- Pause/resume training\n",
    "- Training progress monitoring\n",
    "\n",
    "We'll train a simple neural network on the classic **Iris dataset** using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146ec9b",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's install the required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2df1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch scikit-learn\n",
    "\n",
    "from imgui_bundle import immapp, imgui, implot, hello_imgui\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f0687",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "Load and prepare the Iris dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f12b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset prepared\n",
      "  Training samples: 120\n",
      "  Test samples: 30\n",
      "  Features: 4\n",
      "  Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"✓ Dataset prepared\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904b80d",
   "metadata": {},
   "source": [
    "## Define the Neural Network\n",
    "\n",
    "Create a simple feedforward neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "776372e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Neural network created\n",
      "  Model: IrisNet(\n",
      "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class IrisNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=16, num_classes=3):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = IrisNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✓ Neural network created\")\n",
    "print(f\"  Model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6443eb4",
   "metadata": {},
   "source": [
    "## Training State\n",
    "\n",
    "Create a shared state object to hold training data and hyperparameters. This will be accessed by both the GUI and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14cf3899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training state initialized\n"
     ]
    }
   ],
   "source": [
    "# Training state (shared between GUI and training loop)\n",
    "training_state = {\n",
    "    # Hyperparameters\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \n",
    "    # Training data\n",
    "    \"losses\": [],\n",
    "    \"accuracies\": [],\n",
    "    \"test_accuracies\": [],\n",
    "    \"epochs_completed\": 0,\n",
    "    \"max_epochs\": 10000,\n",
    "    \"max_points\": 10000,  # Maximum points to show in plot\n",
    "    \n",
    "    # Control\n",
    "    \"paused\": False,\n",
    "    \"reset_requested\": False,\n",
    "    \"optimizer\": None,  # Will be created when training starts\n",
    "}\n",
    "\n",
    "print(\"✓ Training state initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2f9d7",
   "metadata": {},
   "source": [
    "## GUI Implementation\n",
    "\n",
    "Define the GUI that displays training progress and allows hyperparameter adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "124e932b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GUI defined\n"
     ]
    }
   ],
   "source": [
    "def training_gui():\n",
    "    \"\"\"GUI that shows training progress and controls\"\"\"\n",
    "    \n",
    "    imgui.text(\"Neural Network Training Monitor\")\n",
    "    \n",
    "    # Training info\n",
    "    imgui.separator_text(\"Training info\")\n",
    "    imgui.text(f\"Epoch: {training_state['epochs_completed']} / {training_state['max_epochs']}\")\n",
    "    \n",
    "    if len(training_state['losses']) > 0:\n",
    "        current_loss = training_state['losses'][-1]\n",
    "        imgui.text(f\"Current Loss: {current_loss:.4f}\")\n",
    "        \n",
    "    if len(training_state['accuracies']) > 0:\n",
    "        train_acc = training_state['accuracies'][-1]\n",
    "        imgui.text(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "        \n",
    "    if len(training_state['test_accuracies']) > 0:\n",
    "        test_acc = training_state['test_accuracies'][-1]\n",
    "        imgui.text(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    imgui.separator()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    imgui.separator_text(\"Tune Hyperparameters\")\n",
    "\n",
    "    # max epochs slider\n",
    "    changed, training_state[\"max_epochs\"] = imgui.slider_int(\n",
    "        \"Max Epochs\", \n",
    "        training_state[\"max_epochs\"], \n",
    "        1, 50000\n",
    "    )\n",
    "\n",
    "    # Max points slider\n",
    "    changed, training_state[\"max_points\"] = imgui.slider_int(\n",
    "        \"Max Points in Plot\", \n",
    "        training_state[\"max_points\"], \n",
    "        100, 50000\n",
    "    )\n",
    "    \n",
    "    # Learning rate slider\n",
    "    changed, new_lr = imgui.slider_float(\n",
    "        \"Learning Rate\", \n",
    "        training_state[\"learning_rate\"], \n",
    "        0.0001, 0.1,\n",
    "        format=\"%.4f\",\n",
    "        flags=imgui.SliderFlags_.logarithmic\n",
    "    )\n",
    "    if changed:\n",
    "        training_state[\"learning_rate\"] = new_lr\n",
    "        # Update optimizer if it exists\n",
    "        if training_state[\"optimizer\"] is not None:\n",
    "            for param_group in training_state[\"optimizer\"].param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "    \n",
    "    # Momentum slider\n",
    "    changed, new_momentum = imgui.slider_float(\n",
    "        \"Momentum\", \n",
    "        training_state[\"momentum\"], \n",
    "        0.0, 0.99,\n",
    "        format=\"%.2f\"\n",
    "    )\n",
    "    if changed:\n",
    "        training_state[\"momentum\"] = new_momentum\n",
    "        # Update optimizer if it exists\n",
    "        if training_state[\"optimizer\"] is not None:\n",
    "            for param_group in training_state[\"optimizer\"].param_groups:\n",
    "                param_group['momentum'] = new_momentum\n",
    "    \n",
    "\n",
    "    imgui.separator_text(\"Controls\")    \n",
    "    # Control buttons\n",
    "    if imgui.button(\"Pause\" if not training_state[\"paused\"] else \"Resume\"):\n",
    "        training_state[\"paused\"] = not training_state[\"paused\"]\n",
    "    \n",
    "    imgui.same_line()\n",
    "    if imgui.button(\"Reset Training\"):\n",
    "        training_state[\"reset_requested\"] = True\n",
    "    \n",
    "    imgui.same_line()\n",
    "    if imgui.button(\"Close\"):\n",
    "        hello_imgui.get_runner_params().app_shall_exit = True\n",
    "    \n",
    "    imgui.separator_text(\"Plots\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    if len(training_state[\"losses\"]) > 0:\n",
    "        if implot.begin_plot(\"Training Loss\", hello_imgui.em_to_vec2(40, 12)):\n",
    "            implot.setup_axes(\"Epoch\", \"Loss\", \n",
    "                            implot.AxisFlags_.auto_fit, \n",
    "                            implot.AxisFlags_.auto_fit)\n",
    "            \n",
    "            # Get data to plot\n",
    "            losses = training_state[\"losses\"][-training_state[\"max_points\"]:]\n",
    "            x_data = np.arange(len(losses), dtype=np.float32)\n",
    "            y_data = np.array(losses, dtype=np.float32)\n",
    "            \n",
    "            implot.plot_line(\"Loss\", x_data, y_data)\n",
    "            implot.end_plot()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    if len(training_state[\"accuracies\"]) > 0:\n",
    "        if implot.begin_plot(\"Accuracy\", hello_imgui.em_to_vec2(40, 12)):\n",
    "            implot.setup_axes(\"Epoch\", \"Accuracy (%)\", \n",
    "                            implot.AxisFlags_.auto_fit, \n",
    "                            implot.AxisFlags_.auto_fit)\n",
    "            \n",
    "            # Get data to plot\n",
    "            train_accs = training_state[\"accuracies\"][-training_state[\"max_points\"]:]\n",
    "            test_accs = training_state[\"test_accuracies\"][-training_state[\"max_points\"]:]\n",
    "            x_data = np.arange(len(train_accs), dtype=np.float32)\n",
    "            \n",
    "            implot.plot_line(\"Train\", x_data, np.array(train_accs, dtype=np.float32))\n",
    "            implot.plot_line(\"Test\", x_data, np.array(test_accs, dtype=np.float32))\n",
    "            implot.end_plot()\n",
    "\n",
    "print(\"✓ GUI defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db7926",
   "metadata": {},
   "source": [
    "## Start the GUI\n",
    "\n",
    "Launch the GUI in non-blocking mode. The window will appear on top of your browser and will update in real-time as training progresses.\n",
    "\n",
    ":::{important}\n",
    "The GUI will open in a separate window on top of the browser. You won't see it embedded in the notebook.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4b02faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GUI started!\n",
      "✓ Run the next cell to start training.\n"
     ]
    }
   ],
   "source": [
    "# Start the GUI (non-blocking)\n",
    "immapp.nb.start(\n",
    "    training_gui,\n",
    "    window_title=\"Neural Network Training\",\n",
    "    window_size=(900, 700),\n",
    "    with_implot=True\n",
    ")\n",
    "\n",
    "print(\"✓ GUI started!\")\n",
    "print(\"✓ Run the next cell to start training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f638a",
   "metadata": {},
   "source": [
    "## Run the Training Loop\n",
    "\n",
    "This cell runs the actual training loop. The model trains for the specified number of epochs, and you can see the loss curve updating in real-time in the GUI window.\n",
    "\n",
    "**While training:**\n",
    "- Adjust the learning rate or momentum sliders to see how they affect training\n",
    "- Pause and resume training\n",
    "- Reset training to start over\n",
    "\n",
    ":::{note}\n",
    "This cell will run for several seconds. The training loop calls `await asyncio.sleep(0)` to yield control to the event loop, allowing the GUI to update smoothly.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de2e1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Resetting training...\n",
      "Epoch 50: Loss=1.0527, Train Acc=40.00%, Test Acc=40.00%\n",
      "Epoch 100: Loss=1.0019, Train Acc=64.17%, Test Acc=66.67%\n",
      "Epoch 150: Loss=0.9488, Train Acc=65.83%, Test Acc=70.00%\n",
      "Epoch 200: Loss=0.8935, Train Acc=65.83%, Test Acc=70.00%\n",
      "Epoch 250: Loss=0.8381, Train Acc=65.83%, Test Acc=70.00%\n",
      "Epoch 300: Loss=0.7855, Train Acc=65.83%, Test Acc=70.00%\n",
      "Epoch 350: Loss=0.7389, Train Acc=65.83%, Test Acc=70.00%\n",
      "Epoch 400: Loss=0.6989, Train Acc=66.67%, Test Acc=70.00%\n",
      "Epoch 450: Loss=0.6643, Train Acc=67.50%, Test Acc=70.00%\n",
      "Epoch 500: Loss=0.6342, Train Acc=67.50%, Test Acc=73.33%\n",
      "Epoch 550: Loss=0.6078, Train Acc=69.17%, Test Acc=73.33%\n",
      "Epoch 600: Loss=0.5844, Train Acc=70.00%, Test Acc=73.33%\n",
      "Epoch 650: Loss=0.5636, Train Acc=71.67%, Test Acc=76.67%\n",
      "Epoch 700: Loss=0.5447, Train Acc=72.50%, Test Acc=80.00%\n",
      "Epoch 750: Loss=0.5277, Train Acc=73.33%, Test Acc=80.00%\n",
      "Epoch 800: Loss=0.5121, Train Acc=74.17%, Test Acc=80.00%\n",
      "Epoch 850: Loss=0.4976, Train Acc=77.50%, Test Acc=80.00%\n",
      "Epoch 900: Loss=0.4841, Train Acc=79.17%, Test Acc=80.00%\n",
      "Epoch 950: Loss=0.4715, Train Acc=81.67%, Test Acc=80.00%\n",
      "Epoch 1000: Loss=0.4594, Train Acc=81.67%, Test Acc=80.00%\n",
      "Epoch 1050: Loss=0.4479, Train Acc=81.67%, Test Acc=80.00%\n",
      "Epoch 1100: Loss=0.4368, Train Acc=84.17%, Test Acc=83.33%\n",
      "Epoch 1150: Loss=0.4260, Train Acc=85.00%, Test Acc=83.33%\n",
      "Epoch 1200: Loss=0.4155, Train Acc=86.67%, Test Acc=83.33%\n",
      "Epoch 1250: Loss=0.4053, Train Acc=86.67%, Test Acc=86.67%\n",
      "Epoch 1300: Loss=0.3954, Train Acc=87.50%, Test Acc=86.67%\n",
      "Epoch 1350: Loss=0.3858, Train Acc=87.50%, Test Acc=86.67%\n",
      "Epoch 1400: Loss=0.3764, Train Acc=89.17%, Test Acc=86.67%\n",
      "Epoch 1450: Loss=0.3673, Train Acc=89.17%, Test Acc=86.67%\n",
      "Epoch 1500: Loss=0.3585, Train Acc=90.83%, Test Acc=90.00%\n",
      "Epoch 1550: Loss=0.3498, Train Acc=91.67%, Test Acc=90.00%\n",
      "Epoch 1600: Loss=0.3413, Train Acc=92.50%, Test Acc=90.00%\n",
      "Epoch 1650: Loss=0.3329, Train Acc=92.50%, Test Acc=90.00%\n",
      "Epoch 1700: Loss=0.3247, Train Acc=92.50%, Test Acc=90.00%\n",
      "Epoch 1750: Loss=0.3168, Train Acc=92.50%, Test Acc=90.00%\n",
      "Epoch 1800: Loss=0.3090, Train Acc=92.50%, Test Acc=93.33%\n",
      "Epoch 1850: Loss=0.3014, Train Acc=92.50%, Test Acc=93.33%\n",
      "Epoch 1900: Loss=0.2940, Train Acc=92.50%, Test Acc=93.33%\n",
      "Epoch 1950: Loss=0.2868, Train Acc=92.50%, Test Acc=93.33%\n",
      "Epoch 2000: Loss=0.2797, Train Acc=93.33%, Test Acc=93.33%\n",
      "Epoch 2050: Loss=0.2728, Train Acc=94.17%, Test Acc=96.67%\n",
      "Epoch 2100: Loss=0.2660, Train Acc=94.17%, Test Acc=96.67%\n",
      "Epoch 2150: Loss=0.2594, Train Acc=94.17%, Test Acc=96.67%\n",
      "Epoch 2200: Loss=0.2529, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2250: Loss=0.2466, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2300: Loss=0.2404, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2350: Loss=0.2344, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2400: Loss=0.2285, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2450: Loss=0.2228, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2500: Loss=0.2173, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2550: Loss=0.2119, Train Acc=95.00%, Test Acc=100.00%\n",
      "Epoch 2600: Loss=0.2067, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2650: Loss=0.2016, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2700: Loss=0.1967, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2750: Loss=0.1919, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2800: Loss=0.1873, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2850: Loss=0.1828, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2900: Loss=0.1785, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 2950: Loss=0.1744, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3000: Loss=0.1703, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3050: Loss=0.1664, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3100: Loss=0.1626, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3150: Loss=0.1590, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3200: Loss=0.1555, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3250: Loss=0.1521, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3300: Loss=0.1489, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3350: Loss=0.1457, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3400: Loss=0.1427, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3450: Loss=0.1398, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3500: Loss=0.1370, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3550: Loss=0.1343, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3600: Loss=0.1318, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3650: Loss=0.1293, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3700: Loss=0.1269, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3750: Loss=0.1246, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3800: Loss=0.1224, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3850: Loss=0.1203, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 3900: Loss=0.1183, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 3950: Loss=0.1164, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4000: Loss=0.1145, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4050: Loss=0.1127, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4100: Loss=0.1109, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4150: Loss=0.1092, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4200: Loss=0.1076, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4250: Loss=0.1060, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4300: Loss=0.1045, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4350: Loss=0.1031, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4400: Loss=0.1017, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4450: Loss=0.1004, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4500: Loss=0.0991, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4550: Loss=0.0978, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4600: Loss=0.0966, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4650: Loss=0.0954, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4700: Loss=0.0943, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4750: Loss=0.0933, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4800: Loss=0.0922, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4850: Loss=0.0912, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4900: Loss=0.0902, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 4950: Loss=0.0893, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5000: Loss=0.0884, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5050: Loss=0.0875, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5100: Loss=0.0867, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5150: Loss=0.0858, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5200: Loss=0.0850, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5250: Loss=0.0843, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5300: Loss=0.0835, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5350: Loss=0.0828, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5400: Loss=0.0821, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5450: Loss=0.0814, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5500: Loss=0.0807, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5550: Loss=0.0801, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5600: Loss=0.0795, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5650: Loss=0.0789, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5700: Loss=0.0783, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5750: Loss=0.0777, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5800: Loss=0.0772, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5850: Loss=0.0766, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5900: Loss=0.0761, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 5950: Loss=0.0756, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6000: Loss=0.0751, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6050: Loss=0.0746, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6100: Loss=0.0741, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6150: Loss=0.0737, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6200: Loss=0.0732, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6250: Loss=0.0728, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6300: Loss=0.0723, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6350: Loss=0.0719, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6400: Loss=0.0715, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6450: Loss=0.0711, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6500: Loss=0.0707, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6550: Loss=0.0703, Train Acc=95.83%, Test Acc=100.00%\n",
      "Epoch 6600: Loss=0.0700, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 6650: Loss=0.0696, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 6700: Loss=0.0693, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 6750: Loss=0.0689, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 6800: Loss=0.0686, Train Acc=96.67%, Test Acc=100.00%\n",
      "Epoch 6850: Loss=0.0682, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 6900: Loss=0.0679, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 6950: Loss=0.0676, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7000: Loss=0.0673, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7050: Loss=0.0670, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7100: Loss=0.0667, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7150: Loss=0.0664, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7200: Loss=0.0661, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7250: Loss=0.0658, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7300: Loss=0.0655, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7350: Loss=0.0653, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7400: Loss=0.0650, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7450: Loss=0.0648, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7500: Loss=0.0645, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7550: Loss=0.0642, Train Acc=97.50%, Test Acc=100.00%\n",
      "Epoch 7600: Loss=0.0640, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7650: Loss=0.0638, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7700: Loss=0.0635, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7750: Loss=0.0633, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7800: Loss=0.0631, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7850: Loss=0.0628, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7900: Loss=0.0626, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 7950: Loss=0.0624, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8000: Loss=0.0622, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8050: Loss=0.0620, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8100: Loss=0.0618, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8150: Loss=0.0616, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8200: Loss=0.0614, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8250: Loss=0.0612, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8300: Loss=0.0610, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8350: Loss=0.0608, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8400: Loss=0.0607, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8450: Loss=0.0605, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8500: Loss=0.0603, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8550: Loss=0.0601, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8600: Loss=0.0600, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8650: Loss=0.0598, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8700: Loss=0.0596, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8750: Loss=0.0595, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8800: Loss=0.0593, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8850: Loss=0.0591, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8900: Loss=0.0590, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 8950: Loss=0.0588, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9000: Loss=0.0587, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9050: Loss=0.0585, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9100: Loss=0.0584, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9150: Loss=0.0582, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9200: Loss=0.0581, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9250: Loss=0.0580, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9300: Loss=0.0578, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9350: Loss=0.0577, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9400: Loss=0.0575, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9450: Loss=0.0574, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9500: Loss=0.0573, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9550: Loss=0.0571, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9600: Loss=0.0570, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9650: Loss=0.0569, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9700: Loss=0.0568, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9750: Loss=0.0566, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9800: Loss=0.0565, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9850: Loss=0.0564, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9900: Loss=0.0563, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 9950: Loss=0.0562, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10000: Loss=0.0560, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10050: Loss=0.0559, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10100: Loss=0.0558, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10150: Loss=0.0557, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10200: Loss=0.0556, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10250: Loss=0.0555, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10300: Loss=0.0554, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10350: Loss=0.0553, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10400: Loss=0.0552, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10450: Loss=0.0551, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10500: Loss=0.0550, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10550: Loss=0.0549, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10600: Loss=0.0548, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10650: Loss=0.0547, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10700: Loss=0.0546, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10750: Loss=0.0545, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10800: Loss=0.0544, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10850: Loss=0.0543, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10900: Loss=0.0542, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 10950: Loss=0.0541, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11000: Loss=0.0540, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11050: Loss=0.0539, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11100: Loss=0.0538, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11150: Loss=0.0537, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11200: Loss=0.0536, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11250: Loss=0.0535, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11300: Loss=0.0534, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11350: Loss=0.0534, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11400: Loss=0.0533, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11450: Loss=0.0532, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11500: Loss=0.0531, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11550: Loss=0.0530, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11600: Loss=0.0529, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11650: Loss=0.0529, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11700: Loss=0.0528, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11750: Loss=0.0527, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11800: Loss=0.0526, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11850: Loss=0.0525, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11900: Loss=0.0525, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 11950: Loss=0.0524, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12000: Loss=0.0523, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12050: Loss=0.0522, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12100: Loss=0.0522, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12150: Loss=0.0521, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12200: Loss=0.0520, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12250: Loss=0.0519, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12300: Loss=0.0519, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12350: Loss=0.0518, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12400: Loss=0.0517, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12450: Loss=0.0516, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12500: Loss=0.0516, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12550: Loss=0.0515, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12600: Loss=0.0514, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12650: Loss=0.0514, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12700: Loss=0.0513, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12750: Loss=0.0512, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12800: Loss=0.0512, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12850: Loss=0.0511, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12900: Loss=0.0510, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 12950: Loss=0.0510, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13000: Loss=0.0509, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13050: Loss=0.0508, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13100: Loss=0.0508, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13150: Loss=0.0507, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13200: Loss=0.0507, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13250: Loss=0.0506, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13300: Loss=0.0505, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13350: Loss=0.0505, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13400: Loss=0.0504, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13450: Loss=0.0503, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13500: Loss=0.0503, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13550: Loss=0.0502, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13600: Loss=0.0502, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13650: Loss=0.0501, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13700: Loss=0.0501, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13750: Loss=0.0500, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13800: Loss=0.0499, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13850: Loss=0.0499, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13900: Loss=0.0498, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 13950: Loss=0.0498, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14000: Loss=0.0497, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14050: Loss=0.0497, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14100: Loss=0.0496, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14150: Loss=0.0496, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14200: Loss=0.0495, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14250: Loss=0.0495, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14300: Loss=0.0494, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14350: Loss=0.0494, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14400: Loss=0.0493, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14450: Loss=0.0493, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14500: Loss=0.0492, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14550: Loss=0.0492, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14600: Loss=0.0491, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14650: Loss=0.0491, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14700: Loss=0.0490, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14750: Loss=0.0490, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14800: Loss=0.0489, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14850: Loss=0.0489, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14900: Loss=0.0488, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 14950: Loss=0.0488, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15000: Loss=0.0487, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15050: Loss=0.0487, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15100: Loss=0.0486, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15150: Loss=0.0486, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15200: Loss=0.0485, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15250: Loss=0.0485, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15300: Loss=0.0485, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15350: Loss=0.0484, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15400: Loss=0.0484, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15450: Loss=0.0483, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15500: Loss=0.0483, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15550: Loss=0.0482, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15600: Loss=0.0482, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15650: Loss=0.0482, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15700: Loss=0.0481, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15750: Loss=0.0481, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15800: Loss=0.0480, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15850: Loss=0.0480, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15900: Loss=0.0479, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 15950: Loss=0.0479, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16000: Loss=0.0479, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16050: Loss=0.0478, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16100: Loss=0.0478, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16150: Loss=0.0477, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16200: Loss=0.0477, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16250: Loss=0.0477, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16300: Loss=0.0476, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16350: Loss=0.0476, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16400: Loss=0.0475, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16450: Loss=0.0475, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16500: Loss=0.0475, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16550: Loss=0.0474, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16600: Loss=0.0474, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16650: Loss=0.0474, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16700: Loss=0.0473, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16750: Loss=0.0473, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16800: Loss=0.0472, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16850: Loss=0.0472, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16900: Loss=0.0472, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 16950: Loss=0.0471, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17000: Loss=0.0471, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17050: Loss=0.0471, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17100: Loss=0.0470, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17150: Loss=0.0470, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17200: Loss=0.0470, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17250: Loss=0.0469, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17300: Loss=0.0469, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17350: Loss=0.0468, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17400: Loss=0.0468, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17450: Loss=0.0468, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17500: Loss=0.0467, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17550: Loss=0.0467, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17600: Loss=0.0467, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17650: Loss=0.0466, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17700: Loss=0.0466, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17750: Loss=0.0466, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17800: Loss=0.0465, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17850: Loss=0.0465, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17900: Loss=0.0465, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 17950: Loss=0.0464, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18000: Loss=0.0464, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18050: Loss=0.0464, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18100: Loss=0.0463, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18150: Loss=0.0463, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18200: Loss=0.0463, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18250: Loss=0.0462, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18300: Loss=0.0462, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18350: Loss=0.0462, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18400: Loss=0.0461, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18450: Loss=0.0461, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18500: Loss=0.0461, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18550: Loss=0.0461, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18600: Loss=0.0460, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18650: Loss=0.0460, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18700: Loss=0.0460, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18750: Loss=0.0459, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18800: Loss=0.0459, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18850: Loss=0.0459, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18900: Loss=0.0458, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 18950: Loss=0.0458, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19000: Loss=0.0458, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19050: Loss=0.0457, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19100: Loss=0.0457, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19150: Loss=0.0457, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19200: Loss=0.0457, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19250: Loss=0.0456, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19300: Loss=0.0456, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19350: Loss=0.0456, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19400: Loss=0.0455, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19450: Loss=0.0455, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19500: Loss=0.0455, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19550: Loss=0.0455, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19600: Loss=0.0454, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19650: Loss=0.0454, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19700: Loss=0.0454, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19750: Loss=0.0453, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19800: Loss=0.0453, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19850: Loss=0.0453, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19900: Loss=0.0452, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 19950: Loss=0.0452, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20000: Loss=0.0452, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20050: Loss=0.0452, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20100: Loss=0.0451, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20150: Loss=0.0451, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20200: Loss=0.0451, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20250: Loss=0.0450, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20300: Loss=0.0450, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20350: Loss=0.0450, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20400: Loss=0.0450, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20450: Loss=0.0449, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20500: Loss=0.0449, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20550: Loss=0.0449, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20600: Loss=0.0448, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20650: Loss=0.0448, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20700: Loss=0.0448, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20750: Loss=0.0448, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20800: Loss=0.0447, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20850: Loss=0.0447, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20900: Loss=0.0447, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 20950: Loss=0.0447, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 21000: Loss=0.0446, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 21050: Loss=0.0446, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 21100: Loss=0.0446, Train Acc=98.33%, Test Acc=100.00%\n",
      "Epoch 21150: Loss=0.0446, Train Acc=98.33%, Test Acc=100.00%\n",
      "✓ Training completed!\n",
      "  Final Loss: 0.0445\n",
      "  Final Train Accuracy: 98.33%\n",
      "  Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "async def train_model():\n",
    "    \"\"\"Training loop that updates GUI in real-time\"\"\"\n",
    "    global model, criterion\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    training_state[\"optimizer\"] = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_state[\"learning_rate\"],\n",
    "        momentum=training_state[\"momentum\"]\n",
    "    )\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < training_state[\"max_epochs\"]: # and immapp.nb.is_running():\n",
    "        # Check if reset requested\n",
    "        if training_state[\"reset_requested\"]:\n",
    "            print(\"Resetting training...\")\n",
    "            model = IrisNet()  # Reinitialize model\n",
    "            training_state[\"losses\"].clear()\n",
    "            training_state[\"accuracies\"].clear()\n",
    "            training_state[\"test_accuracies\"].clear()\n",
    "            training_state[\"epochs_completed\"] = 0\n",
    "            training_state[\"optimizer\"] = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=training_state[\"learning_rate\"],\n",
    "                momentum=training_state[\"momentum\"]\n",
    "            )\n",
    "            training_state[\"reset_requested\"] = False\n",
    "            epoch = 0\n",
    "            continue\n",
    "        \n",
    "        # Skip if paused\n",
    "        if training_state[\"paused\"]:\n",
    "            await asyncio.sleep(0.1)\n",
    "            continue\n",
    "        \n",
    "        # Training step\n",
    "        model.train()\n",
    "        training_state[\"optimizer\"].zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        training_state[\"optimizer\"].step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_acc = (predicted == y_train_tensor).sum().item() / len(y_train_tensor) * 100\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor)\n",
    "            _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "            test_acc = (test_predicted == y_test_tensor).sum().item() / len(y_test_tensor) * 100\n",
    "        \n",
    "        # Update training state\n",
    "        training_state[\"losses\"].append(loss.item())\n",
    "        training_state[\"accuracies\"].append(train_acc)\n",
    "        training_state[\"test_accuracies\"].append(test_acc)\n",
    "        training_state[\"epochs_completed\"] = epoch + 1\n",
    "        \n",
    "        # Keep buffer size limited\n",
    "        if len(training_state[\"losses\"]) > training_state[\"max_points\"]:\n",
    "            training_state[\"losses\"].pop(0)\n",
    "            training_state[\"accuracies\"].pop(0)\n",
    "            training_state[\"test_accuracies\"].pop(0)\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "        # Yield control to event loop (allows GUI to update)\n",
    "        # (Call this more often if you want a more responsive GUI)\n",
    "        await asyncio.sleep(0)\n",
    "        \n",
    "        # Print progress every 50 epochs\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"✓ Training completed!\")\n",
    "    print(f\"  Final Loss: {training_state['losses'][-1]:.4f}\")\n",
    "    print(f\"  Final Train Accuracy: {training_state['accuracies'][-1]:.2f}%\")\n",
    "    print(f\"  Final Test Accuracy: {training_state['test_accuracies'][-1]:.2f}%\")\n",
    "\n",
    "# Run the training loop\n",
    "await train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275e96c",
   "metadata": {},
   "source": [
    "## Experiment with Hyperparameters\n",
    "\n",
    "While the training is running, you can:\n",
    "\n",
    "1. **Adjust the learning rate** - Try increasing it to see faster convergence (but possibly less stable), or decreasing it for more stable but slower training.\n",
    "\n",
    "2. **Modify momentum** - Change the momentum parameter to see how it affects training dynamics.\n",
    "\n",
    "3. **Pause and resume** - Pause training, adjust parameters, then resume.\n",
    "\n",
    "4. **Reset training** - Click \"Reset Training\" to start from scratch with new hyperparameters.\n",
    "\n",
    "Try running the training cell again and experimenting with different values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ae51c",
   "metadata": {},
   "source": [
    "## Check Training Status\n",
    "\n",
    "You can check if the training is still running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07436662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immapp.nb.is_running()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ef7b6",
   "metadata": {},
   "source": [
    "## Stop the GUI\n",
    "\n",
    "When you're done, stop the GUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587a0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "immapp.nb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9685c",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Model\n",
    "\n",
    "After training, you can evaluate the final model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "531d756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FINAL MODEL PERFORMANCE\n",
      "==================================================\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n",
      "==================================================\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Class names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Training set\n",
    "    train_outputs = model(X_train_tensor)\n",
    "    _, train_predicted = torch.max(train_outputs.data, 1)\n",
    "    train_accuracy = (train_predicted == y_train_tensor).sum().item() / len(y_train_tensor) * 100\n",
    "    \n",
    "    # Test set\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    test_accuracy = (test_predicted == y_test_tensor).sum().item() / len(y_test_tensor) * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_tensor.numpy(), test_predicted.numpy())\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(cm)\n",
    "print(f\"\\nClass names: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73552850",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✓ **Real-time training visualization** - Loss and accuracy curves updated live during training\n",
    "\n",
    "✓ **Interactive hyperparameter tuning** - Adjusted learning rate and momentum on-the-fly\n",
    "\n",
    "✓ **Async training loop** - Used `await asyncio.sleep(0)` to yield control and keep GUI responsive\n",
    "\n",
    "✓ **Training control** - Paused, resumed, and reset training from the GUI\n",
    "\n",
    "This pattern can be extended to more complex scenarios:\n",
    "- Larger datasets and deeper networks\n",
    "- Additional hyperparameters (batch size, learning rate schedules, etc.)\n",
    "- Multiple metrics (precision, recall, F1-score)\n",
    "- Training on GPU\n",
    "- Saving/loading checkpoints during training\n",
    "\n",
    "The key is the non-blocking async pattern: the GUI runs in one task while the training loop runs in another, both cooperating through shared state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v314 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
